{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc38ba2-d07c-4c80-abde-52933337d0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regression of one time series (ts) onto a field with lead lag years. field leads ts\n",
    "# by '-ly' years. And do block bootstrap significant test \n",
    "# the output is for one lead-lag year: lead year '-ly'\n",
    "\n",
    "length=6 # -5,....0 year\n",
    "ly = 0\n",
    "variable = 'windy'\n",
    "block_length=2\n",
    "n_resamples=1000\n",
    "confidence_level=0.95\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# read data\n",
    "data = xr.open_dataset('/work/uo1075/u241321/data/vas_1969-2019_assi_dt.nc',decode_times=False)\n",
    "ts = np.load(\"/work/uo1075/u241321/data/eemd_t310_assi_Norwegian.npy\")[2,:]\n",
    "\n",
    "corr      = xr.open_dataset(\"/work/uo1075/u241321/data/reg_\"+variable+\"_T_c2_44.nc\")['__xarray_dataarray_variable__'] \n",
    "\n",
    "\n",
    "nyear=ts.size\n",
    "var = np.mean(data['__xarray_dataarray_variable__'], axis=1)\n",
    "var_1 = var.stack(spatial=('lat','lon')).dropna(dim=\"spatial\") #time,space\n",
    "\n",
    "y1 = var_1[length-1+ly:nyear+ly-1]\n",
    "y2 = ts[length-1:nyear-1]\n",
    "\n",
    "coef = corr[5+ly,:,:].stack(spatial=('lat','lon'))\n",
    "\n",
    "\n",
    "# regression, center on 4-47, 44 year (start from 0)\n",
    "\n",
    "# for m in range(0,field.shape[1],1):\n",
    "#         coe[0,m] = regression(y[5:49], field[0:44,m])\n",
    "#         coe[1,m] = regression(y[5:49], field[1:45,m])\n",
    "#         coe[2,m] = regression(y[5:49], field[2:46,m])\n",
    "#         coe[3,m] = regression(y[5:49], field[3:47,m])\n",
    "#         coe[4,m] = regression(y[5:49], field[4:48,m])\n",
    "#         coe[5,m] = regression(y[5:49], field[5:49,m])\n",
    " \n",
    "# Due to technical issue, here use 44 years. Cut the last year.\n",
    "\n",
    "#  LinearRegression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def regression(x,y):\n",
    "\n",
    "    coef = LinearRegression(fit_intercept=True).fit(x.reshape(-1, 1), y.values.reshape(-1, 1)).coef_[0,0]\n",
    "    \n",
    "\n",
    "    return coef\n",
    "\n",
    "\n",
    "# bootstrap\n",
    "\n",
    "def _bootstrap_resample(sample1, sample2, block_length, n_resamples=None, random_state=None):\n",
    "    \"\"\"Bootstrap resample the sample.\"\"\"\n",
    "   \n",
    "    np.random.shuffle(sample1.reshape((-1,block_length)))\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(sample2.values.reshape((-1,block_length)))\n",
    "    \n",
    "    # resamples = sample\n",
    "    coef = regression(sample1, sample2)\n",
    "    # bootstrap - each row is a random resample of original observations\n",
    "    # i = rng_integers(random_state, 0, n, (n_resamples, n))\n",
    "\n",
    "    # resamples = sample[..., i]\n",
    "    return coef\n",
    "\n",
    "\n",
    "def bootstrap(data1, data2, rawcoef, block_length, confidence_level, n_resamples,\n",
    "              method, random_state=None):\n",
    " \n",
    "    # the 1st dimension of data is time\n",
    "    # confidence_level: 0.9, 0.95, 0.99...\n",
    "    # n_resamples: 1000, 10000...\n",
    "    # method: 'percentile', 'basic', 'bca'\n",
    "    \n",
    "    # the original Pearson correlation coefficients \n",
    "    coef_raw = rawcoef\n",
    " \n",
    "    theta_hat_b = []\n",
    "\n",
    "    batch_nominal = n_resamples\n",
    "\n",
    "    for k in range(0, n_resamples):\n",
    "        # batch_actual = min(batch_nominal, n_resamples-k)\n",
    "        # Generate resamples\n",
    "        resampled_coef_data = []\n",
    "        # for sample in data:\n",
    "        resample = _bootstrap_resample(data1, data2, block_length, \n",
    "                                           random_state=random_state)\n",
    "        resampled_coef_data.append(resample)\n",
    "\n",
    "        # Compute bootstrap distribution of Pearson correlation coefficients\n",
    "        theta_hat_b.append(resampled_coef_data)\n",
    "    theta_hat_b = np.concatenate(theta_hat_b, axis=-1)\n",
    "\n",
    "    # Calculate percentile interval\n",
    "    alpha = (1 - confidence_level)/2\n",
    "    if method == 'bca':\n",
    "        interval = _bca_interval(data, statistic, axis=-1, alpha=alpha,\n",
    "                                 theta_hat_b=theta_hat_b, batch=batch)\n",
    "        percentile_fun = _percentile_along_axis\n",
    "    else:\n",
    "        interval = alpha, 1-alpha\n",
    "\n",
    "        def percentile_fun(a, q):\n",
    "            return np.percentile(a=a, q=q, axis=-1)\n",
    "\n",
    "    # Calculate confidence interval of statistic\n",
    "    ci_l = percentile_fun(theta_hat_b, interval[0]*100)\n",
    "    ci_u = percentile_fun(theta_hat_b, interval[1]*100)\n",
    "    if method == 'basic':  # see [3]\n",
    "        theta_hat = statistic(*data, axis=-1)\n",
    "        ci_l, ci_u = 2*theta_hat - ci_u, 2*theta_hat - ci_l\n",
    "\n",
    "      \n",
    "    \n",
    "    if coef_raw >= ci_u:\n",
    "        sig = 1\n",
    "    else:\n",
    "        if coef_raw <= ci_l:\n",
    "            sig = -1\n",
    "        else:\n",
    "            sig = 0\n",
    "        \n",
    "    return coef_raw ,sig\n",
    "\n",
    "# apply to global filed\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "from itertools import starmap\n",
    "\n",
    "def boots_gridpoint(m):\n",
    "    \n",
    "    mode = bootstrap(\n",
    "         y2, y1[:,m], coef[m], block_length=block_length, confidence_level=confidence_level, \n",
    "        n_resamples=n_resamples, method='percentile'\n",
    "    )\n",
    "\n",
    "    return mode\n",
    "\n",
    "\n",
    "# pool = Pool()\n",
    "# ! python --version\n",
    "\n",
    "res = Pool().map(boots_gridpoint,np.arange(0,var.lat.size*var.lon.size,1))\n",
    "re = np.array(res)\n",
    "\n",
    "# output \n",
    "corr = re[:,0].reshape((var.lat.size, var.lon.size))\n",
    "sig = re[:,1].reshape((var.lat.size, var.lon.size))\n",
    "\n",
    "lat = data['lat']\n",
    "lon = data['lon']\n",
    "nlat = lat.size\n",
    "nlon = lon.size\n",
    "\n",
    "boot = xr.Dataset(data_vars={'lat': ([\"lat\"], lat.data),\n",
    "                              'lon':  ([\"lon\"], lon.data),\n",
    "                              'sig': ([\"lat\", \"lon\"], sig),\n",
    "                              'corr': ([\"lat\", \"lon\"], corr),})\n",
    "# corr = correlation[\"corr\"]\n",
    "boot.to_netcdf(\"/work/uo1075/u241321/data/boot_reg_\"+variable+\"_ly\"+str(-ly)+\"_block\"+str(block_length)+\"_new.nc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f3818-f00b-43ea-8b3d-3ddcf467ebe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1 Python 3 (based on the module python3/2023.01)",
   "language": "python",
   "name": "python3_2023_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
